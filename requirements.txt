# Core dependencies for the fine-tuning and inference scripts

# PyTorch (adjust version based on your CUDA version if using GPU, or use CPU version)
# Find instructions at: https://pytorch.org/get-started/locally/
# For Apple Silicon Macs:
torch

# Hugging Face libraries
transformers>=4.40.0
peft>=0.10.0 # Parameter-Efficient Fine-Tuning
datasets>=2.18.0 # For loading data
accelerate>=0.29.0 # Simplifies distributed training and hardware management

# For QLoRA (quantization)
# bitsandbytes>=0.43.0  # DISABLED: Not compatible with macOS ARM64 (Apple Silicon)
# Note: When running on a compatible Linux/Windows machine with CUDA, uncomment this line
# On macOS, we'll fall back to other LoRA methods without quantization

# Optional, for better terminal input in inference script
# readline # DISABLED: Build issues on macOS - the built-in readline is sufficient

# For argument parsing (built-in)
# argparse

# Consider adding:
sentencepiece # Often required by tokenizers
protobuf # Often required by tokenizers
# wandb # If you want experiment tracking (remove `report_to="none"` in TrainingArguments)
# TRL (Transformer Reinforcement Learning) # For more advanced fine-tuning like SFTTrainer, DPO, etc. 